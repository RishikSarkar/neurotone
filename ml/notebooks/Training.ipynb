{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a710cd5-3054-49de-97f1-ba712f00b893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import WavLMModel, WavLMConfig, AutoFeatureExtractor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "import random\n",
    "import math\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "308b5b7d-bc92-43c7-b9c5-67ad5206c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b102fbec-99c2-4da0-b331-b210809561c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root directory: ..\n",
      "Metadata directory: ..\\data\\processed\n",
      "Audio base directory: ..\\data\\combined\n",
      "Output directory for checkpoints/plots: ..\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Paths\n",
    "# Assuming this notebook is in 'ml/' or 'ml/notebooks/' relative to project root\n",
    "root_dir = Path('../')\n",
    "data_dir = root_dir / 'data'\n",
    "\n",
    "# Directory where the FINAL training metadata CSVs are stored\n",
    "metadata_dir = data_dir / 'processed'\n",
    "\n",
    "# Directory where the FINAL segmented/processed audio WAVs are stored\n",
    "# This is the base for paths listed in the metadata CSVs\n",
    "audio_base_dir = data_dir / 'combined' # Changed from processed_dir\n",
    "\n",
    "# Directory for saving model checkpoints and plots\n",
    "output_dir = data_dir / 'processed' # Or choose a different output dir like 'training_outputs'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Root directory: {root_dir}\")\n",
    "print(f\"Metadata directory: {metadata_dir}\")\n",
    "print(f\"Audio base directory: {audio_base_dir}\")\n",
    "print(f\"Output directory for checkpoints/plots: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f49977ad-fd84-4e9b-9619-bd0dcbb831d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e33caef-4307-4251-96b2-67f4452040a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['No Dementia', 'Dementia']\n",
    "num_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a11c917f-9923-483f-94f7-c2fedb964c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: DementiaAudioDataset (Modified for new structure)\n",
    "\n",
    "class DementiaAudioDataset(Dataset):\n",
    "    def __init__(self, metadata_df, audio_base_directory, feature_extractor, max_length=160000, augment=False):\n",
    "        \"\"\"\n",
    "        Dataset for binary audio dementia classification using pre-processed data.\n",
    "\n",
    "        Args:\n",
    "            metadata_df: DataFrame containing 'relative_audio_path' and 'label' columns.\n",
    "            audio_base_directory: Path object pointing to the base directory\n",
    "                                    (e.g., 'data/combined') where relative_audio_path starts.\n",
    "            feature_extractor: WavLM feature extractor.\n",
    "            max_length: Maximum audio length in samples (e.g., 10s @ 16kHz = 160000).\n",
    "            augment: Whether to apply augmentation during training.\n",
    "        \"\"\"\n",
    "        self.df = metadata_df\n",
    "        self.audio_base_dir = Path(audio_base_directory) # Ensure it's a Path object\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "        self.sampling_rate = 16000 # Target sampling rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _load_audio(self, audio_path: Path):\n",
    "        # Load audio\n",
    "        if not audio_path.is_file():\n",
    "             print(f\"ERROR: Audio file not found at: {audio_path}\")\n",
    "             return np.zeros(self.max_length) # Return zeros if file missing\n",
    "\n",
    "        try:\n",
    "            # Use soundfile for potentially faster WAV loading\n",
    "            waveform, sr = sf.read(audio_path, dtype='float32')\n",
    "            # Verify sample rate after loading\n",
    "            if sr != self.sampling_rate:\n",
    "                 print(f\"Warning: File {audio_path.name} has incorrect sample rate {sr}, expected {self.sampling_rate}. Skipping or add resampling.\")\n",
    "                 # Option 1: Skip - return zeros\n",
    "                 # return np.zeros(self.max_length)\n",
    "                 # Option 2: Resample (requires librosa or torchaudio) - Using librosa for simplicity here\n",
    "                 # waveform = librosa.resample(waveform, orig_sr=sr, target_sr=self.sampling_rate)\n",
    "                 # --- Let's assume preprocessing handled this, return zeros if mismatch ---\n",
    "                 print(f\"ERROR: Mismatched sample rate for {audio_path.name}. Expected {self.sampling_rate}, got {sr}.\")\n",
    "                 return np.zeros(self.max_length)\n",
    "\n",
    "            # Ensure mono (soundfile loads mono as 1D, stereo as 2D)\n",
    "            if waveform.ndim > 1:\n",
    "                # Simple average for stereo to mono conversion\n",
    "                waveform = np.mean(waveform, axis=1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file {audio_path}: {e}\")\n",
    "            return np.zeros(self.max_length) # Return zeros as a fallback\n",
    "\n",
    "        # Apply augmentation if requested\n",
    "        if self.augment:\n",
    "            # Librosa augmentations require sample rate argument if not loaded with librosa\n",
    "            sr_for_augment = self.sampling_rate\n",
    "            # Random volume change\n",
    "            if random.random() < 0.3:\n",
    "                # Normalize first to apply gain relative to RMS/peak\n",
    "                norm_factor = np.max(np.abs(waveform))\n",
    "                if norm_factor > 1e-6: # Avoid division by zero\n",
    "                    waveform_norm = waveform / norm_factor\n",
    "                else:\n",
    "                    waveform_norm = waveform\n",
    "                db_change = random.uniform(-10, 10)\n",
    "                waveform = waveform_norm * (10 ** (db_change / 20)) * norm_factor\n",
    "\n",
    "            # Pitch shift (can be slow)\n",
    "            if random.random() < 0.5:\n",
    "                semitones = random.uniform(-4, 4)\n",
    "                waveform = librosa.effects.pitch_shift(y=waveform, sr=sr_for_augment, n_steps=semitones)\n",
    "\n",
    "            # Time stretch (can be slow, changes length)\n",
    "            if random.random() < 0.5:\n",
    "                rate = random.uniform(0.85, 1.15) # Slightly smaller range\n",
    "                try:\n",
    "                    waveform = librosa.effects.time_stretch(y=waveform, rate=rate)\n",
    "                except Exception as ts_err:\n",
    "                    print(f\"Warning: Time stretch failed for a sample: {ts_err}\")\n",
    "\n",
    "\n",
    "        # Pad or truncate AFTER potential time stretch\n",
    "        current_len = len(waveform)\n",
    "        if current_len > self.max_length:\n",
    "            waveform = waveform[:self.max_length]\n",
    "        elif current_len < self.max_length:\n",
    "            padding = self.max_length - current_len\n",
    "            waveform = np.pad(waveform, (0, padding), 'constant')\n",
    "        # else: length is exactly max_length\n",
    "\n",
    "        return waveform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        relative_path = row['relative_audio_path']\n",
    "        label = row['label']\n",
    "\n",
    "        # Construct the full path using the base directory\n",
    "        # Ensure relative_path doesn't start with a slash if base_dir is used\n",
    "        full_audio_path = self.audio_base_dir / relative_path\n",
    "\n",
    "        # Load and preprocess audio\n",
    "        waveform = self._load_audio(full_audio_path)\n",
    "\n",
    "        # Convert to tensor\n",
    "        # Need to handle potential all-zero waveform if loading failed\n",
    "        if np.all(waveform == 0):\n",
    "             print(f\"Warning: Waveform is all zeros for index {idx}, path {full_audio_path}. Check loading errors.\")\n",
    "             # Create zero inputs for the model to handle gracefully downstream if needed\n",
    "             inputs = self.feature_extractor(waveform, sampling_rate=self.sampling_rate, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length) # Use FE padding? might not match exactly\n",
    "             # Or manually create zero tensor of expected feature size if known? Less robust.\n",
    "        else:\n",
    "             inputs = self.feature_extractor(waveform, sampling_rate=self.sampling_rate, return_tensors=\"pt\")\n",
    "\n",
    "        # Squeeze batch dimension added by feature extractor\n",
    "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}\n",
    "\n",
    "        # Return label as float for BCEWithLogitsLoss\n",
    "        return inputs, torch.tensor(label, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5667fde-b333-41f6-9216-984d1891ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WavLMForDementiaClassification(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes=4):\n",
    "        super(WavLMForDementiaClassification, self).__init__()\n",
    "        \n",
    "        # Load pre-trained WavLM model\n",
    "        self.wavlm = WavLMModel.from_pretrained(base_model_name)\n",
    "        hidden_size = self.wavlm.config.hidden_size\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1) # Output 1 logit for binary classification\n",
    "        )\n",
    "    \n",
    "    def freeze_feature_extractor(self):\n",
    "        \"\"\"Freeze the feature extraction part of the model\"\"\"\n",
    "        for param in self.wavlm.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_feature_extractor(self):\n",
    "        \"\"\"Unfreeze the feature extraction part of the model\"\"\"\n",
    "        for param in self.wavlm.feature_extractor.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def forward(self, input_values, attention_mask=None):\n",
    "        outputs = self.wavlm(input_values=input_values, attention_mask=attention_mask)\n",
    "        \n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfc14b0f-e7ab-470e-8704-42b09177a085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: train_model function (Updated)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, num_epochs, device, phase_name, start_epoch=0, best_val_f1=0.0):\n",
    "    \"\"\"Train the model on the training set and evaluate on validation set.\n",
    "       Handles starting from a specific epoch and best F1 score for checkpointing.\"\"\"\n",
    "    criterion = nn.BCEWithLogitsLoss() # REMOVED pos_weight as data is balanced\n",
    "\n",
    "    # best_val_f1 is now passed as argument, initialized outside if not resuming\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_f1s, val_f1s = [], []\n",
    "\n",
    "    print(f\"Starting training phase {phase_name} from Epoch {start_epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        current_epoch = epoch + 1\n",
    "        print(f\"Epoch {current_epoch}/{num_epochs}\")\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        y_true_train, y_pred_train = [], []\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            input_values = inputs['input_values'].to(device)\n",
    "            attention_mask = inputs.get('attention_mask', None)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(torch.bool).to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Use autocast for mixed precision\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                outputs = model(input_values=input_values, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "            # Scaler is used for backward pass\n",
    "            # loss.backward() # Replace with scaler\n",
    "            # optimizer.step() # Replace with scaler\n",
    "            # --- AMP Changes Start ---\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            # --- AMP Changes End ---\n",
    "\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            predictions = (probabilities > 0.5).squeeze(1).long()\n",
    "            y_true_train.extend(labels.cpu().numpy())\n",
    "            y_pred_train.extend(predictions.cpu().numpy())\n",
    "\n",
    "\n",
    "        # Compute epoch metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        train_f1 = f1_score(y_true_train, y_pred_train, average='binary', pos_label=1, zero_division=0)\n",
    "        train_losses.append(train_loss)\n",
    "        train_f1s.append(train_f1)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        y_true_val, y_pred_val = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "                input_values = inputs['input_values'].to(device)\n",
    "                attention_mask = inputs.get('attention_mask', None)\n",
    "                if attention_mask is not None:\n",
    "                    attention_mask = attention_mask.to(torch.bool).to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Use autocast for validation too (consistency, though not strictly needed for grads)\n",
    "                with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                    outputs = model(input_values=input_values, attention_mask=attention_mask)\n",
    "                    loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                probabilities = torch.sigmoid(outputs)\n",
    "                predictions = (probabilities > 0.5).squeeze(1).long()\n",
    "                y_true_val.extend(labels.cpu().numpy())\n",
    "                y_pred_val.extend(predictions.cpu().numpy())\n",
    "\n",
    "\n",
    "        # Compute epoch metrics\n",
    "        val_loss /= len(val_loader)\n",
    "        val_f1 = f1_score(y_true_val, y_pred_val, average='binary', pos_label=1, zero_division=0)\n",
    "        # val_precision = precision_score(y_true_val, y_pred_val, average='binary', pos_label=1, zero_division=0) # Can add back if needed\n",
    "        # val_recall = recall_score(y_true_val, y_pred_val, average='binary', pos_label=1, zero_division=0) # Can add back if needed\n",
    "        val_losses.append(val_loss)\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        # --- Checkpoint Saving Logic ---\n",
    "        is_best = val_f1 >= best_val_f1\n",
    "        if is_best:\n",
    "            old_best_f1 = best_val_f1\n",
    "            best_val_f1 = val_f1\n",
    "\n",
    "            checkpoint = {\n",
    "                'epoch': current_epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                'best_val_f1': best_val_f1,\n",
    "                'phase_name': phase_name,\n",
    "                'scaler_state_dict': scaler.state_dict() # --- Add scaler state ---\n",
    "            }\n",
    "            # Use output_dir defined in Cell 3 for saving\n",
    "            checkpoint_path = output_dir / f'checkpoint_{phase_name}_final_data.pt' # Updated filename\n",
    "\n",
    "            print(f\"Validation F1 improved or matched ({old_best_f1:.4f} --> {best_val_f1:.4f}). Saving checkpoint to {checkpoint_path}\")\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "            # Detailed evaluation metrics printout (keep as is)\n",
    "            # ... (print metrics logic - check confusion matrix index safety) ...\n",
    "            print(\"Best model state updated! Detailed validation metrics (Binary):\")\n",
    "            accuracy = accuracy_score(y_true_val, y_pred_val)\n",
    "            precision_pos = precision_score(y_true_val, y_pred_val, pos_label=1, zero_division=0)\n",
    "            recall_pos = recall_score(y_true_val, y_pred_val, pos_label=1, zero_division=0)\n",
    "            f1_pos = f1_score(y_true_val, y_pred_val, pos_label=1, zero_division=0)\n",
    "            precision_neg = precision_score(y_true_val, y_pred_val, pos_label=0, zero_division=0)\n",
    "            recall_neg = recall_score(y_true_val, y_pred_val, pos_label=0, zero_division=0)\n",
    "            f1_neg = f1_score(y_true_val, y_pred_val, pos_label=0, zero_division=0)\n",
    "            conf_matrix = confusion_matrix(y_true_val, y_pred_val, labels=[0, 1])\n",
    "            print(f\"Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"Dementia (Pos) Precision: {precision_pos:.4f}\")\n",
    "            print(f\"Dementia (Pos) Recall: {recall_pos:.4f}\")\n",
    "            print(f\"Dementia (Pos) F1: {f1_pos:.4f}\")\n",
    "            print(f\"No Dementia (Neg) Precision: {precision_neg:.4f}\")\n",
    "            print(f\"No Dementia (Neg) Recall: {recall_neg:.4f}\")\n",
    "            print(f\"No Dementia (Neg) F1: {f1_neg:.4f}\")\n",
    "            print(\"Confusion Matrix (Rows=True, Cols=Pred):\")\n",
    "            print(\"  Pred: NoDem Dem\")\n",
    "            print(f\"True:\")\n",
    "            # Safer printing for confusion matrix\n",
    "            cm_shape = conf_matrix.shape\n",
    "            print(f\"NoDem   {conf_matrix[0,0] if cm_shape[0]>0 and cm_shape[1]>0 else 0:<3} {conf_matrix[0,1] if cm_shape[0]>0 and cm_shape[1]>1 else 0:<3}\")\n",
    "            print(f\"Dem     {conf_matrix[1,0] if cm_shape[0]>1 and cm_shape[1]>0 else 0:<3} {conf_matrix[1,1] if cm_shape[0]>1 and cm_shape[1]>1 else 0:<3}\")\n",
    "\n",
    "\n",
    "        # --- Update learning rate AFTER saving checkpoint ---\n",
    "        if scheduler:\n",
    "            # Note: Scheduler step might need adjustment depending on AMP scaler state,\n",
    "            # but typically called after optimizer step/update. Check specific scheduler docs if issues arise.\n",
    "             scheduler.step()\n",
    "\n",
    "\n",
    "        # Print epoch summary (always print)\n",
    "        print(f\"Epoch {current_epoch} Summary: Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f} | Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "        # --- End Epoch Loop ---\n",
    "\n",
    "\n",
    "    # Plot training and validation metrics\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    # Adjust x-axis range for plotting\n",
    "    epochs_run = range(start_epoch, num_epochs)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_run, train_losses, label=\"Train\")\n",
    "    plt.plot(epochs_run, val_losses, label=\"Validation\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_run, train_f1s, label=\"Train\")\n",
    "    plt.plot(epochs_run, val_f1s, label=\"Validation\")\n",
    "    plt.title(\"F1 Score (Dementia Class)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Use output_dir for saving plots\n",
    "    plt.savefig(output_dir / f'training_metrics_{phase_name}_final_data.png') # Updated filename\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # --- Load the Best Model state from the saved checkpoint file ---\n",
    "    best_checkpoint_path = output_dir / f'checkpoint_{phase_name}_final_data.pt'\n",
    "    if best_checkpoint_path.exists():\n",
    "        print(f\"Loading best model state from checkpoint: {best_checkpoint_path}\")\n",
    "        # Load checkpoint onto the correct device\n",
    "        checkpoint = torch.load(best_checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Best model loaded (Epoch {checkpoint.get('epoch', '?')}, Val F1: {checkpoint.get('best_val_f1', '?'):.4f})\")\n",
    "    else:\n",
    "        print(f\"Warning: No checkpoint found at {best_checkpoint_path}. Returning model in its current state (after last epoch).\")\n",
    "\n",
    "    return model # Return the model loaded with the best state found during this run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecd96a28-fbc9-4ea2-a341-cc1f826620b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: plot_confusion_matrix (Updated save path)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names): # class_names will be ['No Dementia', 'Dementia']\n",
    "    # Ensure labels cover the full range if only one class is predicted\n",
    "    labels_present = sorted(list(set(y_true) | set(y_pred)))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1]) # Explicitly use labels=[0, 1]\n",
    "    plt.figure(figsize=(6, 5)) # Smaller figure for 2x2\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    # Use output_dir for saving plots and new filename\n",
    "    plt.savefig(output_dir / 'confusion_matrix_final_data.png') # Use output_dir and updated name\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fff4b454-20c3-41fa-ac09-a5b8be705537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FINAL balanced & segmented metadata splits...\n",
      "Loaded Full Train metadata: 39440 samples ({0: 19720, 1: 19720})\n",
      "Loaded Full Validation metadata: 4930 samples ({1: 2465, 0: 2465})\n",
      "Loaded Test metadata: 4930 samples ({0: 2465, 1: 2465})\n",
      "Using 10.0% sample of Train data: 3944 samples ({1: 1972, 0: 1972})\n",
      "Using 10.0% sample of Validation data: 493 samples ({1: 247, 0: 246})\n",
      "\n",
      "Loading feature extractor from microsoft/wavlm-base-plus\n",
      "\n",
      "Creating datasets...\n",
      "Using DataLoader with persistent_workers=True and prefetch_factor=2\n",
      "\n",
      "Final DataLoaders:\n",
      "Train samples: 3944 (Sampled)\n",
      "Validation samples: 493 (Sampled)\n",
      "Test samples: 4930\n",
      "Batch Size: 16\n",
      "Num Workers: 0\n",
      "Estimated batches per training epoch: 247\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Load FINAL Balanced Data, Create Datasets/Loaders (Increased Batch Size & Workers)\n",
    "\n",
    "print(\"Loading FINAL balanced & segmented metadata splits...\")\n",
    "# Use metadata_dir defined in Cell 3\n",
    "train_metadata_path = metadata_dir / 'train_meta_len_filtered_balanced.csv'\n",
    "val_metadata_path = metadata_dir / 'val_meta_len_filtered_balanced.csv'\n",
    "test_metadata_path = metadata_dir / 'test_meta_len_filtered_balanced.csv'\n",
    "\n",
    "try:\n",
    "    # --- Load Full Data ---\n",
    "    df_train_full = pd.read_csv(train_metadata_path)\n",
    "    df_val_full = pd.read_csv(val_metadata_path)\n",
    "    df_test = pd.read_csv(test_metadata_path) # Keep full test set for now\n",
    "\n",
    "    print(f\"Loaded Full Train metadata: {len(df_train_full)} samples ({df_train_full['label'].value_counts().to_dict()})\")\n",
    "    print(f\"Loaded Full Validation metadata: {len(df_val_full)} samples ({df_val_full['label'].value_counts().to_dict()})\")\n",
    "    print(f\"Loaded Test metadata: {len(df_test)} samples ({df_test['label'].value_counts().to_dict()})\")\n",
    "\n",
    "    # --- Sample 10% of Train and Validation Data ---\n",
    "    train_sample_frac = 0.10\n",
    "    val_sample_frac = 0.10\n",
    "\n",
    "    # Use stratified sampling to maintain class balance\n",
    "    from sklearn.model_selection import train_test_split # Ensure this is imported\n",
    "\n",
    "    # Sample training data\n",
    "    if len(df_train_full) > 1: # Need at least 2 samples for train_test_split\n",
    "         df_train, _ = train_test_split(\n",
    "             df_train_full,\n",
    "             train_size=train_sample_frac,\n",
    "             random_state=42,\n",
    "             stratify=df_train_full['label']\n",
    "         )\n",
    "         print(f\"Using {train_sample_frac*100:.1f}% sample of Train data: {len(df_train)} samples ({df_train['label'].value_counts().to_dict()})\")\n",
    "    else:\n",
    "         print(\"Warning: Full training dataset too small to sample.\")\n",
    "         df_train = df_train_full # Use the full (tiny) set\n",
    "\n",
    "    # Sample validation data\n",
    "    if len(df_val_full) > 1:\n",
    "         df_val, _ = train_test_split(\n",
    "             df_val_full,\n",
    "             train_size=val_sample_frac,\n",
    "             random_state=42,\n",
    "             stratify=df_val_full['label']\n",
    "         )\n",
    "         print(f\"Using {val_sample_frac*100:.1f}% sample of Validation data: {len(df_val)} samples ({df_val['label'].value_counts().to_dict()})\")\n",
    "    else:\n",
    "         print(\"Warning: Full validation dataset too small to sample.\")\n",
    "         df_val = df_val_full # Use the full (tiny) set\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "     print(f\"ERROR: Could not load final metadata files: {e}\")\n",
    "     print(\"Please ensure the 'Prepare_Control_Data...' notebook ran successfully and generated these files.\")\n",
    "     raise\n",
    "except Exception as e:\n",
    "     print(f\"Error loading metadata: {e}\")\n",
    "     raise\n",
    "\n",
    "\n",
    "# --- Remove pos_weight calculation ---\n",
    "# Data is now balanced by undersampling controls in the prep script.\n",
    "# No change needed here for sampling\n",
    "\n",
    "\n",
    "# Load feature extractor\n",
    "base_model_name = \"microsoft/wavlm-base-plus\"\n",
    "print(f\"\\nLoading feature extractor from {base_model_name}\")\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(base_model_name)\n",
    "\n",
    "# Create datasets using the FINAL metadata DataFrames\n",
    "# Pass the audio_base_dir to the Dataset constructor\n",
    "print(\"\\nCreating datasets...\")\n",
    "train_dataset = DementiaAudioDataset(\n",
    "    metadata_df=df_train, # Use the SAMPLED training dataframe\n",
    "    audio_base_directory=audio_base_dir, # Pass base dir for audio files\n",
    "    feature_extractor=feature_extractor,\n",
    "    max_length=160000,\n",
    "    augment=True\n",
    ")\n",
    "val_dataset = DementiaAudioDataset(\n",
    "    metadata_df=df_val, # Use the SAMPLED validation dataframe\n",
    "    audio_base_directory=audio_base_dir, # Pass base dir\n",
    "    feature_extractor=feature_extractor,\n",
    "    max_length=160000,\n",
    "    augment=False\n",
    ")\n",
    "test_dataset = DementiaAudioDataset(\n",
    "    metadata_df=df_test, # Use the FULL test dataframe\n",
    "    audio_base_directory=audio_base_dir, # Pass base dir\n",
    "    feature_extractor=feature_extractor,\n",
    "    max_length=160000,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "# --- Increased batch size and num_workers ---\n",
    "batch_size = 16 # Increased from 16 (adjust based on GPU memory)\n",
    "num_workers = 0 # Increased from 2 (adjust based on CPU cores/RAM) <-- You should INCREASE this later!\n",
    "# --- End of changes ---\n",
    "\n",
    "# Recommended DataLoader settings for CUDA:\n",
    "# - num_workers > 0 enables parallel loading\n",
    "# - pin_memory=True speeds up CPU->GPU transfer if using GPU\n",
    "# - persistent_workers=True (PyTorch 1.7+) keeps workers alive between epochs, reduces startup overhead\n",
    "# - prefetch_factor (PyTorch 1.7+) controls how many batches workers prepare ahead\n",
    "try:\n",
    "     # Use newer args if available\n",
    "     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True if device=='cuda' else False, persistent_workers=True if num_workers>0 else False, prefetch_factor=2 if num_workers>0 else None)\n",
    "     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True if device=='cuda' else False, persistent_workers=True if num_workers>0 else False, prefetch_factor=2 if num_workers>0 else None)\n",
    "     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True if device=='cuda' else False, persistent_workers=True if num_workers>0 else False, prefetch_factor=2 if num_workers>0 else None)\n",
    "     print(\"Using DataLoader with persistent_workers=True and prefetch_factor=2\")\n",
    "except TypeError:\n",
    "     # Fallback for older PyTorch versions\n",
    "     print(\"Warning: persistent_workers/prefetch_factor not supported by this PyTorch version. Using basic DataLoader.\")\n",
    "     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True if device=='cuda' else False)\n",
    "     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True if device=='cuda' else False)\n",
    "     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True if device=='cuda' else False)\n",
    "\n",
    "\n",
    "print(f\"\\nFinal DataLoaders:\")\n",
    "print(f\"Train samples: {len(train_dataset)} (Sampled)\") # Updated print statement\n",
    "print(f\"Validation samples: {len(val_dataset)} (Sampled)\") # Updated print statement\n",
    "print(f\"Test samples: {len(test_dataset)}\") # Kept full test set\n",
    "print(f\"Batch Size: {batch_size}\")\n",
    "print(f\"Num Workers: {num_workers}\") # Reminder: Increase this later\n",
    "\n",
    "# Estimate new number of batches\n",
    "new_train_batches = math.ceil(len(train_dataset) / batch_size)\n",
    "print(f\"Estimated batches per training epoch: {new_train_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7064458b-31f0-45e2-a8cb-dae6f3847c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model with base model: microsoft/wavlm-base-plus\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initializing model with base model: {base_model_name}\")\n",
    "model = WavLMForDementiaClassification(base_model_name, num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "285f6f28-864d-4a95-9777-acb1b2956880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMP GradScaler Initialized (Device: cuda, Enabled: True)\n"
     ]
    }
   ],
   "source": [
    "# Cell 13.5: Initialize AMP GradScaler\n",
    "\n",
    "# Initialize GradScaler for Automatic Mixed Precision\n",
    "# Enable only if device is cuda\n",
    "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "print(f\"AMP GradScaler Initialized (Device: {device.type}, Enabled: {scaler.is_enabled()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff64f702-ff31-413e-857d-3c648964391d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints found. Starting training from scratch.\n",
      "\n",
      "--- Starting/Resuming Phase 1 (Epochs 1 to 3) ---\n",
      "Starting training phase phase1_binary from Epoch 1/3\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                 | 0/247 [00:00<?, ?it/s]D:\\Cornell\\Academic\\Spring 2025\\Startup Studio\\MVP\\neurotone\\ml\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Training:  18%|█▍      | 45/247 [11:05<48:28, 14.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Waveform is all zeros for index 1954, path ..\\data\\combined\\dementia\\fc67a330-af0a-4f5a-8cc7-f84ac5865bea.wav. Check loading errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|███▏   | 114/247 [27:51<34:37, 15.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Waveform is all zeros for index 1129, path ..\\data\\combined\\dementia\\088db7f0-b6fd-4880-b27e-d36778003350.wav. Check loading errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|███▋   | 131/247 [31:55<28:04, 14.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Waveform is all zeros for index 799, path ..\\data\\combined\\dementia\\6d25e59a-7da6-4433-bf2d-2f285c5e68f1.wav. Check loading errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|██████▊| 239/247 [58:44<01:53, 14.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Waveform is all zeros for index 430, path ..\\data\\combined\\dementia\\87b5b1ca-1a28-49b5-9dc2-07c898fa66a9.wav. Check loading errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|████▉| 245/247 [1:00:38<00:38, 19.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Waveform is all zeros for index 3173, path ..\\data\\combined\\dementia\\c395e2c3-d33f-4fbc-85d2-164065f0c71d.wav. Check loading errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████| 247/247 [1:01:12<00:00, 14.87s/it]\n",
      "Validation:   0%| | 0/31 [00:00<?, D:\\Cornell\\Academic\\Spring 2025\\Startup Studio\\MVP\\neurotone\\ml\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Validation:  65%|▋| 20/31 [02:06<01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Waveform is all zeros for index 320, path ..\\data\\combined\\dementia\\9e7f83f2-9518-4e0d-9df6-8476cf5754e7.wav. Check loading errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█| 31/31 [02:56<00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 improved or matched (0.0000 --> 0.9899). Saving checkpoint to ..\\data\\processed\\checkpoint_phase1_binary_final_data.pt\n",
      "Best model state updated! Detailed validation metrics (Binary):\n",
      "Accuracy: 0.9899\n",
      "Dementia (Pos) Precision: 0.9919\n",
      "Dementia (Pos) Recall: 0.9879\n",
      "Dementia (Pos) F1: 0.9899\n",
      "No Dementia (Neg) Precision: 0.9879\n",
      "No Dementia (Neg) Recall: 0.9919\n",
      "No Dementia (Neg) F1: 0.9899\n",
      "Confusion Matrix (Rows=True, Cols=Pred):\n",
      "  Pred: NoDem Dem\n",
      "True:\n",
      "NoDem   244 2  \n",
      "Dem     3   244\n",
      "Epoch 1 Summary: Train Loss: 0.6048, Train F1: 0.7986 | Val Loss: 0.4099, Val F1: 0.9899\n",
      "--------------------------------------------------\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%| | 0/247 [00:00<?, ?D:\\Cornell\\Academic\\Spring 2025\\Startup Studio\\MVP\\neurotone\\ml\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Training:  34%|▎| 84/247 [23:58<52:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Waveform is all zeros for index 430, path ..\\data\\combined\\dementia\\87b5b1ca-1a28-49b5-9dc2-07c898fa66a9.wav. Check loading errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|▍| 115/247 [33:23<39"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Waveform is all zeros for index 1954, path ..\\data\\combined\\dementia\\fc67a330-af0a-4f5a-8cc7-f84ac5865bea.wav. Check loading errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|▋| 170/247 [48:59<22"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Waveform is all zeros for index 1129, path ..\\data\\combined\\dementia\\088db7f0-b6fd-4880-b27e-d36778003350.wav. Check loading errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|▊| 213/247 [1:00:59<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Waveform is all zeros for index 799, path ..\\data\\combined\\dementia\\6d25e59a-7da6-4433-bf2d-2f285c5e68f1.wav. Check loading errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|▉| 236/247 [1:08:24<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Waveform is all zeros for index 3173, path ..\\data\\combined\\dementia\\c395e2c3-d33f-4fbc-85d2-164065f0c71d.wav. Check loading errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█| 247/247 [1:11:53<\n",
      "Validation:   0%| | 0/31 [00:00<?, D:\\Cornell\\Academic\\Spring 2025\\Startup Studio\\MVP\\neurotone\\ml\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Validation:  65%|▋| 20/31 [02:04<01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Waveform is all zeros for index 320, path ..\\data\\combined\\dementia\\9e7f83f2-9518-4e0d-9df6-8476cf5754e7.wav. Check loading errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█| 31/31 [03:07<00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 improved or matched (0.9899 --> 0.9899). Saving checkpoint to ..\\data\\processed\\checkpoint_phase1_binary_final_data.pt\n",
      "Best model state updated! Detailed validation metrics (Binary):\n",
      "Accuracy: 0.9899\n",
      "Dementia (Pos) Precision: 0.9919\n",
      "Dementia (Pos) Recall: 0.9879\n",
      "Dementia (Pos) F1: 0.9899\n",
      "No Dementia (Neg) Precision: 0.9879\n",
      "No Dementia (Neg) Recall: 0.9919\n",
      "No Dementia (Neg) F1: 0.9899\n",
      "Confusion Matrix (Rows=True, Cols=Pred):\n",
      "  Pred: NoDem Dem\n",
      "True:\n",
      "NoDem   244 2  \n",
      "Dem     3   244\n",
      "Epoch 2 Summary: Train Loss: 0.3322, Train F1: 0.9639 | Val Loss: 0.1826, Val F1: 0.9899\n",
      "--------------------------------------------------\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%| | 0/247 [00:00<?, ?D:\\Cornell\\Academic\\Spring 2025\\Startup Studio\\MVP\\neurotone\\ml\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Training:  33%|▎| 82/247 [24:10<48:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Waveform is all zeros for index 430, path ..\\data\\combined\\dementia\\87b5b1ca-1a28-49b5-9dc2-07c898fa66a9.wav. Check loading errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|▌| 153/247 [43:24<25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Waveform is all zeros for index 1954, path ..\\data\\combined\\dementia\\fc67a330-af0a-4f5a-8cc7-f84ac5865bea.wav. Check loading errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|▊| 207/247 [58:49<11"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Waveform is all zeros for index 799, path ..\\data\\combined\\dementia\\6d25e59a-7da6-4433-bf2d-2f285c5e68f1.wav. Check loading errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|▊| 210/247 [59:43<10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Waveform is all zeros for index 1129, path ..\\data\\combined\\dementia\\088db7f0-b6fd-4880-b27e-d36778003350.wav. Check loading errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|▉| 222/247 [1:02:49<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Waveform is all zeros for index 3173, path ..\\data\\combined\\dementia\\c395e2c3-d33f-4fbc-85d2-164065f0c71d.wav. Check loading errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|▉| 224/247 [1:03:20<"
     ]
    }
   ],
   "source": [
    "# Cell 14: Phase 1 Training Call (Updated)\n",
    "\n",
    "# --- Checkpoint Loading Logic ---\n",
    "# Define checkpoint paths using output_dir and updated filenames\n",
    "phase1_checkpoint_path = output_dir / 'checkpoint_phase1_binary_final_data.pt'\n",
    "phase2_checkpoint_path = output_dir / 'checkpoint_phase2_binary_final_data.pt'\n",
    "\n",
    "# Variables for resuming\n",
    "start_epoch_phase1 = 0\n",
    "best_val_f1_phase1 = 0.0\n",
    "optimizer_state_phase1 = None\n",
    "scheduler_state_phase1 = None\n",
    "scaler_state_phase1 = None # <-- Add scaler state\n",
    "\n",
    "start_epoch_phase2 = 0\n",
    "best_val_f1_phase2 = 0.0\n",
    "optimizer_state_phase2 = None\n",
    "scheduler_state_phase2 = None\n",
    "scaler_state_phase2 = None # <-- Add scaler state\n",
    "\n",
    "load_checkpoint_path = None\n",
    "if phase2_checkpoint_path.exists():\n",
    "    print(f\"Phase 2 checkpoint found: {phase2_checkpoint_path}. Loading state.\")\n",
    "    load_checkpoint_path = phase2_checkpoint_path\n",
    "elif phase1_checkpoint_path.exists():\n",
    "    print(f\"Phase 1 checkpoint found: {phase1_checkpoint_path}. Loading state.\")\n",
    "    load_checkpoint_path = phase1_checkpoint_path\n",
    "else:\n",
    "    print(\"No checkpoints found. Starting training from scratch.\")\n",
    "\n",
    "if load_checkpoint_path:\n",
    "    try:\n",
    "        checkpoint = torch.load(load_checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        phase_loaded = checkpoint.get('phase_name', None)\n",
    "        epoch_loaded = checkpoint.get('epoch', 0)\n",
    "        best_f1_loaded = checkpoint.get('best_val_f1', 0.0)\n",
    "        opt_state_loaded = checkpoint.get('optimizer_state_dict')\n",
    "        sch_state_loaded = checkpoint.get('scheduler_state_dict')\n",
    "        scl_state_loaded = checkpoint.get('scaler_state_dict') # <-- Load scaler state\n",
    "\n",
    "        print(f\"Checkpoint loaded from Phase: {phase_loaded}, Epoch: {epoch_loaded}, Best Val F1: {best_f1_loaded:.4f}\")\n",
    "\n",
    "        if phase_loaded == \"phase1_binary\":\n",
    "             start_epoch_phase1 = epoch_loaded # Resume AFTER this epoch\n",
    "             best_val_f1_phase1 = best_f1_loaded\n",
    "             optimizer_state_phase1 = opt_state_loaded\n",
    "             scheduler_state_phase1 = sch_state_loaded\n",
    "             scaler_state_phase1 = scl_state_loaded # <-- Store scaler state\n",
    "             if start_epoch_phase1 >= 3: # Check if phase 1 finished\n",
    "                  print(\"Phase 1 seems complete in checkpoint. Preparing for Phase 2.\")\n",
    "                  start_epoch_phase1 = 3 # Ensure phase 1 is skipped\n",
    "\n",
    "        elif phase_loaded == \"phase2_binary\":\n",
    "             start_epoch_phase2 = epoch_loaded # Resume AFTER this epoch\n",
    "             best_val_f1_phase2 = best_f1_loaded\n",
    "             optimizer_state_phase2 = opt_state_loaded\n",
    "             scheduler_state_phase2 = sch_state_loaded\n",
    "             scaler_state_phase2 = scl_state_loaded # <-- Store scaler state\n",
    "             # If resuming phase 2, skip phase 1 completely\n",
    "             start_epoch_phase1 = 3 # Set phase 1 to max epochs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}. Restarting training.\")\n",
    "        # Reset all resume variables\n",
    "        start_epoch_phase1 = 0; best_val_f1_phase1 = 0.0; optimizer_state_phase1 = None; scheduler_state_phase1 = None; scaler_state_phase1 = None;\n",
    "        start_epoch_phase2 = 0; best_val_f1_phase2 = 0.0; optimizer_state_phase2 = None; scheduler_state_phase2 = None; scaler_state_phase2 = None;\n",
    "\n",
    "\n",
    "# --- Phase 1 Training ---\n",
    "num_epochs_phase1 = 3\n",
    "\n",
    "if start_epoch_phase1 < num_epochs_phase1:\n",
    "    print(f\"\\n--- Starting/Resuming Phase 1 (Epochs {start_epoch_phase1 + 1} to {num_epochs_phase1}) ---\")\n",
    "    model.freeze_feature_extractor() # Ensure correct state\n",
    "\n",
    "    # Create optimizer/scheduler for PHASE 1\n",
    "    optimizer_phase1 = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), # Only pass parameters requiring grad\n",
    "        lr=5e-5, weight_decay=0.01\n",
    "    )\n",
    "    # Adjust steps based on remaining epochs\n",
    "    total_steps_phase1 = len(train_loader) * (num_epochs_phase1 - start_epoch_phase1)\n",
    "    warmup_steps_phase1 = int(0.1 * total_steps_phase1) if total_steps_phase1 > 0 else 0\n",
    "    scheduler_phase1 = optim.lr_scheduler.LinearLR(\n",
    "        optimizer_phase1, start_factor=0.1, end_factor=1.0, total_iters=max(1, warmup_steps_phase1)\n",
    "    )\n",
    "\n",
    "    # Load states if resuming this specific phase\n",
    "    if optimizer_state_phase1:\n",
    "        print(\"Loading Phase 1 optimizer state...\")\n",
    "        optimizer_phase1.load_state_dict(optimizer_state_phase1)\n",
    "    if scheduler_state_phase1 and scheduler_phase1:\n",
    "        print(\"Loading Phase 1 scheduler state...\")\n",
    "        scheduler_phase1.load_state_dict(scheduler_state_phase1)\n",
    "    if scaler_state_phase1 and scaler.is_enabled(): # <-- Load scaler state\n",
    "        print(\"Loading Phase 1 GradScaler state...\")\n",
    "        scaler.load_state_dict(scaler_state_phase1)\n",
    "\n",
    "\n",
    "    # Run training for phase 1\n",
    "    model = train_model(\n",
    "        model=model, train_loader=train_loader, val_loader=val_loader,\n",
    "        optimizer=optimizer_phase1, scheduler=scheduler_phase1,\n",
    "        num_epochs=num_epochs_phase1, device=device, phase_name=\"phase1_binary\",\n",
    "        start_epoch=start_epoch_phase1, best_val_f1=best_val_f1_phase1\n",
    "    )\n",
    "else:\n",
    "    print(\"\\n--- Skipping Phase 1 (already completed or resuming Phase 2) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ccf3bd-8c9c-4617-85f7-29bb9ec1bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Phase 2 Training Call (Updated)\n",
    "\n",
    "# --- Phase 2 Training ---\n",
    "num_epochs_phase2 = 5 # More epochs for fine-tuning\n",
    "\n",
    "if start_epoch_phase2 < num_epochs_phase2:\n",
    "    print(f\"\\n--- Starting/Resuming Phase 2 (Epochs {start_epoch_phase2 + 1} to {num_epochs_phase2}) ---\")\n",
    "    model.unfreeze_feature_extractor() # Ensure correct state\n",
    "\n",
    "    # Create optimizer/scheduler for PHASE 2\n",
    "    optimizer_phase2 = optim.AdamW(\n",
    "        model.parameters(), # Train all parameters\n",
    "        lr=1e-5, # Lower LR\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    # Adjust steps based on remaining epochs\n",
    "    total_steps_phase2 = len(train_loader) * (num_epochs_phase2 - start_epoch_phase2)\n",
    "    warmup_steps_phase2 = int(0.1 * total_steps_phase2) if total_steps_phase2 > 0 else 0\n",
    "    scheduler_phase2 = optim.lr_scheduler.LinearLR(\n",
    "        optimizer_phase2, start_factor=0.1, end_factor=1.0, total_iters=max(1, warmup_steps_phase2)\n",
    "    )\n",
    "\n",
    "    # Load states if resuming this specific phase\n",
    "    if optimizer_state_phase2:\n",
    "        print(\"Loading Phase 2 optimizer state...\")\n",
    "        optimizer_phase2.load_state_dict(optimizer_state_phase2)\n",
    "    if scheduler_state_phase2 and scheduler_phase2:\n",
    "        print(\"Loading Phase 2 scheduler state...\")\n",
    "        scheduler_phase2.load_state_dict(scheduler_state_phase2)\n",
    "    if scaler_state_phase2 and scaler.is_enabled(): # <-- Load scaler state\n",
    "        print(\"Loading Phase 2 GradScaler state...\")\n",
    "        scaler.load_state_dict(scaler_state_phase2)\n",
    "\n",
    "    # Run training for phase 2\n",
    "    model = train_model(\n",
    "        model=model, train_loader=train_loader, val_loader=val_loader,\n",
    "        optimizer=optimizer_phase2, scheduler=scheduler_phase2,\n",
    "        num_epochs=num_epochs_phase2, device=device, phase_name=\"phase2_binary\",\n",
    "        start_epoch=start_epoch_phase2, best_val_f1=best_val_f1_phase2\n",
    "    )\n",
    "else:\n",
    "     print(\"\\n--- Skipping Phase 2 (already completed according to checkpoint) ---\")\n",
    "     # If skipping, ensure the BEST model from phase 2 checkpoint is loaded\n",
    "     if phase2_checkpoint_path.exists():\n",
    "          print(\"Loading final Phase 2 model state directly...\")\n",
    "          final_phase2_checkpoint = torch.load(phase2_checkpoint_path, map_location=device)\n",
    "          model.load_state_dict(final_phase2_checkpoint['model_state_dict'])\n",
    "          best_val_f1_phase2 = final_phase2_checkpoint['best_val_f1']\n",
    "          print(f\"Phase 2 Best Val F1: {best_val_f1_phase2:.4f}\")\n",
    "     # If phase 2 checkpoint doesn't exist but phase 1 did, model holds best phase 1 state\n",
    "     elif phase1_checkpoint_path.exists():\n",
    "          print(\"Phase 2 checkpoint not found, model holds best state from Phase 1.\")\n",
    "     else:\n",
    "          print(\"No checkpoints found, model is in initial state.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ebadc2-6fe2-40db-8113-6568f61150f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Final Evaluation (Updated)\n",
    "\n",
    "print(\"\\n--- Evaluating BEST model on test set ---\")\n",
    "# Model should be loaded with the best weights from train_model return\n",
    "\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "y_probs = [] # Store probabilities for potential threshold tuning later\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        input_values = inputs['input_values'].to(device)\n",
    "        attention_mask = inputs.get('attention_mask', None)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(torch.bool).to(device)\n",
    "\n",
    "        # Use autocast for consistency\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            outputs = model(input_values=input_values, attention_mask=attention_mask)\n",
    "\n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "        predictions = (probabilities > 0.5).squeeze(1).long() # Threshold at 0.5\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predictions.cpu().numpy())\n",
    "        y_probs.extend(probabilities.squeeze(1).cpu().numpy()) # Store probabilities\n",
    "\n",
    "# --- Compute and display test metrics ---\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision_pos = precision_score(y_true, y_pred, pos_label=1, average='binary', zero_division=0)\n",
    "recall_pos = recall_score(y_true, y_pred, pos_label=1, average='binary', zero_division=0)\n",
    "f1_pos = f1_score(y_true, y_pred, pos_label=1, average='binary', zero_division=0)\n",
    "f1_neg = f1_score(y_true, y_pred, pos_label=0, average='binary', zero_division=0)\n",
    "# Add ROC AUC if desired\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    roc_auc = roc_auc_score(y_true, y_probs)\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "except ImportError:\n",
    "    print(\"Skipping ROC AUC calculation (sklearn needs update or import error)\")\n",
    "    roc_auc = None\n",
    "\n",
    "print(\"\\n--- Test Set Results (Binary Classification) ---\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Dementia (Pos Class) Precision: {precision_pos:.4f}\")\n",
    "print(f\"Dementia (Pos Class) Recall: {recall_pos:.4f}\")\n",
    "print(f\"Dementia (Pos Class) F1 Score: {f1_pos:.4f}\")\n",
    "print(f\"No Dementia (Neg Class) F1 Score: {f1_neg:.4f}\")\n",
    "\n",
    "\n",
    "# Plot confusion matrix using binary class names\n",
    "plot_confusion_matrix(y_true, y_pred, class_names) # class_names is ['No Dementia', 'Dementia']\n",
    "\n",
    "# --- Save the final model state (best one loaded from training) ---\n",
    "# Save to output_dir with descriptive name\n",
    "final_model_path = output_dir / 'final_model_binary_segmented_balanced.pt'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'test_metrics': {\n",
    "        'accuracy': accuracy,\n",
    "        'precision_dementia': precision_pos,\n",
    "        'recall_dementia': recall_pos,\n",
    "        'f1_dementia': f1_pos,\n",
    "        'f1_nodementia': f1_neg,\n",
    "        'roc_auc': roc_auc\n",
    "    },\n",
    "    # Include info about the data used\n",
    "    'training_metadata': {\n",
    "        'train_file': str(final_train_meta_path.name),\n",
    "        'val_file': str(final_val_meta_path.name),\n",
    "        'test_file': str(final_test_meta_path.name)\n",
    "    }\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"\\nFinal model state saved to: {final_model_path}\")\n",
    "print(\"\\n--- Model training and evaluation completed! ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
